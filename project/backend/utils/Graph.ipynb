{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f2897a2",
   "metadata": {},
   "source": [
    "## Establishing Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d760d832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Neo4j!\n"
     ]
    }
   ],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "uri = \"bolt://localhost:7687\"\n",
    "user = \"neo4j\"\n",
    "password = \"12345678\"  \n",
    "database = \"moviesdb\"\n",
    "\n",
    "driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "def test_connection():\n",
    "    with driver.session(database=database) as session:\n",
    "        result = session.run(\"RETURN 'Connected to Neo4j!' AS msg\")\n",
    "        for record in result:\n",
    "            print(record[\"msg\"])\n",
    "\n",
    "test_connection()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6c7199",
   "metadata": {},
   "source": [
    "## Creating Nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d82ec5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Creating nodes\n",
    "\n",
    "def run_query(query, params=None):\n",
    "    with driver.session(database=\"metadatatagging\") as session:\n",
    "        result = session.run(query, params or {})\n",
    "        return [record.data() for record in result]\n",
    "    \n",
    "def create_movie(tx, imdb_id=-1, title=None, year=-1, \n",
    "                 akas=None, metascore=None, imdb_rating=None, imdb_votes=None, \n",
    "                 budget=None, countries=None, age_restrictions=None, \n",
    "                 keywords=None, taglines=None, plot=None):\n",
    "\n",
    "    query = \"\"\"\n",
    "    MERGE (m:Movie {imdb_id: $imdb_id})\n",
    "    SET m.title = $title,\n",
    "        m.year = $year,\n",
    "        m.akas = $akas,\n",
    "        m.metascore = $metascore,\n",
    "        m.imdb_rating = $imdb_rating,\n",
    "        m.imdb_votes = $imdb_votes,\n",
    "        m.budget = $budget,\n",
    "        m.countries = $countries,\n",
    "        m.age_restrictions = $age_restrictions,\n",
    "        m.keywords = $keywords,\n",
    "        m.taglines = $taglines,\n",
    "        m.plot = $plot\n",
    "    \"\"\"\n",
    "    tx.run(query,\n",
    "           imdb_id=imdb_id, title=title, year=year, akas=akas,\n",
    "           metascore=metascore, imdb_rating=imdb_rating, imdb_votes=imdb_votes,\n",
    "           budget=budget, countries=countries,\n",
    "           age_restrictions=age_restrictions,\n",
    "           keywords=keywords, taglines=taglines,\n",
    "           plot=plot)\n",
    "    \n",
    "\n",
    "\n",
    "def create_genre(tx, name):\n",
    "    tx.run(\"MERGE (g:Genre {name: $name})\", name=name)\n",
    "\n",
    "def create_person(tx, name, role=None):\n",
    "    query = \"\"\"\n",
    "    MERGE (p:Person {name: $name})\n",
    "    SET p.roles = CASE \n",
    "        WHEN p.roles IS NULL THEN [$role]\n",
    "        WHEN NOT $role IN p.roles THEN p.roles + $role\n",
    "        ELSE p.roles\n",
    "    END\n",
    "    \"\"\"\n",
    "    tx.run(query, name=name, role=role)\n",
    "\n",
    "\n",
    "def create_company(tx, name):\n",
    "    tx.run(\"MERGE (c:Company {name: $name})\", name=name)\n",
    "\n",
    "def create_award(tx, name):\n",
    "    tx.run(\"MERGE (a:Award {name: $name})\", name=name)\n",
    "    \n",
    "def create_review(tx, movie_id, review_id, text, reviewer, score=None, sentiment=None, sentiment_score=None):\n",
    "    query = \"\"\"\n",
    "    MERGE (m:Movie {imdb_id: $movie_id})\n",
    "      ON CREATE SET m.title = '-1'\n",
    "    MERGE (r:Review {id: $review_id})\n",
    "    SET r.text = $text,\n",
    "        r.reviewer = $reviewer,\n",
    "        r.score = $score,\n",
    "        r.sentiment = $sentiment,\n",
    "        r.sentiment_score = $sentiment_score\n",
    "    MERGE (m)-[:HAS_REVIEW]->(r)\n",
    "    \"\"\"\n",
    "    tx.run(query, movie_id=movie_id, review_id=review_id, text=text,\n",
    "           reviewer=reviewer, score=score, sentiment=sentiment,\n",
    "           sentiment_score=sentiment_score)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9903e4",
   "metadata": {},
   "source": [
    "## Connecting Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d565d7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting Nodes\n",
    "\n",
    "def connect_movie_genres(tx, imdb_id, genres):\n",
    "    query = \"\"\"\n",
    "    MATCH (m:Movie {imdb_id: $imdb_id})\n",
    "    WITH m, $genres AS genre_list\n",
    "    UNWIND genre_list AS genre_name\n",
    "    MERGE (g:Genre {name: genre_name})\n",
    "    MERGE (m)-[:HAS_GENRE]->(g)\n",
    "    \"\"\"\n",
    "    tx.run(query, imdb_id=imdb_id, genres=genres)\n",
    "\n",
    "def connect_movie_award(tx, imdb_id, title, award_name, rel_type=\"WON\"):\n",
    "    query = f\"\"\"\n",
    "    MERGE (m:Movie {{imdb_id: $imdb_id}})\n",
    "      ON CREATE SET m.title = $title\n",
    "    MATCH (a:Award {{name: $award_name}})\n",
    "    MERGE (m)-[:{rel_type}]->(a)\n",
    "    \"\"\"\n",
    "    tx.run(query, imdb_id=imdb_id, title=title, award_name=award_name)\n",
    "    \n",
    "def connect_movie_company(tx, imdb_id, company_name):\n",
    "    query = \"\"\"\n",
    "    MATCH (m:Movie {imdb_id: $imdb_id})\n",
    "    MERGE (c:Company {name: $company_name})\n",
    "    MERGE (m)-[:PRODUCED_BY]->(c)\n",
    "    \"\"\"\n",
    "    tx.run(query, imdb_id=imdb_id, company_name=company_name)\n",
    "    \n",
    "def connect_movie_person(tx, imdb_id, person_name, rel_type, role=None):\n",
    "    query = f\"\"\"\n",
    "    MATCH (m:Movie {{imdb_id: $imdb_id}})\n",
    "    MERGE (p:Person {{name: $person_name}})\n",
    "    SET p.roles = CASE \n",
    "        WHEN p.roles IS NULL THEN [$role]\n",
    "        WHEN NOT $role IN p.roles THEN p.roles + $role\n",
    "        ELSE p.roles\n",
    "    END\n",
    "    MERGE (m)-[:{rel_type}]->(p)\n",
    "    \"\"\"\n",
    "    tx.run(query, imdb_id=imdb_id, person_name=person_name, role=role)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e413e3",
   "metadata": {},
   "source": [
    "### Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "80d6e019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse for movies metadata\n",
    "\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def _clean_text(s):\n",
    "    if not isinstance(s, str):\n",
    "        return s\n",
    "    s = s.strip()\n",
    "    # remove leading/trailing punctuation often found in your raws (commas, quotes, colons)\n",
    "    s = re.sub(r'^[,:\\s\"\\']+', '', s)\n",
    "    s = re.sub(r'[,:\\s\"\\']+$', '', s)\n",
    "    return s.strip()\n",
    "\n",
    "def parse_akas_to_dict(akas_str):\n",
    "    if akas_str is None or (isinstance(akas_str, float) and pd.isna(akas_str)):\n",
    "        return None\n",
    "    s = str(akas_str).strip()\n",
    "    if not s:\n",
    "        return None\n",
    "\n",
    "    pattern = r\"(.+?)\\s*\\(([^)]+)\\)\"\n",
    "    matches = re.findall(pattern, s)\n",
    "\n",
    "    if not matches:\n",
    "        parts = [p.strip() for p in s.split(\",\") if p.strip()]\n",
    "        if not parts:\n",
    "            return None\n",
    "        return {f\"title_{i+1}\": _clean_text(p) for i, p in enumerate(parts)}\n",
    "\n",
    "    aka_dict = {}\n",
    "    for title, country in matches:\n",
    "        title_clean = _clean_text(title)\n",
    "        country_clean = _clean_text(country)\n",
    "        if not country_clean:\n",
    "            key = f\"unknown_{len(aka_dict)+1}\"\n",
    "        else:\n",
    "            key = country_clean\n",
    "        aka_dict[key] = title_clean\n",
    "\n",
    "    return aka_dict if aka_dict else None\n",
    "\n",
    "def parse_age_restrictions_to_dict(age_str):\n",
    "    if age_str is None or (isinstance(age_str, float) and pd.isna(age_str)):\n",
    "        return None\n",
    "    s = str(age_str).strip()\n",
    "    if not s:\n",
    "        return None\n",
    "\n",
    "    entries = [e.strip() for e in s.split(\",\") if e.strip()]\n",
    "    restrictions = {}\n",
    "    for entry in entries:\n",
    "        m = re.match(r\"([^:]+):([^:]+?)(?:::\\((.+?)\\))?$\", entry)\n",
    "        if m:\n",
    "            country = _clean_text(m.group(1))\n",
    "            rating = _clean_text(m.group(2))\n",
    "            region = m.group(3)\n",
    "            if region:\n",
    "                region_clean = _clean_text(region)\n",
    "                key = f\"{country} ({region_clean})\" if country else region_clean\n",
    "            else:\n",
    "                key = country\n",
    "            if key and rating:\n",
    "                restrictions[key] = rating\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    return restrictions if restrictions else None\n",
    "\n",
    "\n",
    "def parse_taglines(tagline_str): # only the commas followed by capital letter is considered a new element in the list.\n",
    "    if tagline_str is None or (isinstance(tagline_str, float) and pd.isna(tagline_str)):\n",
    "        return None\n",
    "    s = str(tagline_str).strip()\n",
    "    if not s:\n",
    "        return None\n",
    "    # split on comma + space only when next char is uppercase\n",
    "    parts = re.split(r', (?=[A-Z])', s)\n",
    "    taglines = [t.strip() for t in parts if t.strip()]\n",
    "    return taglines if taglines else None\n",
    "\n",
    "def parse_keywords(keyword_str):\n",
    "    if not isinstance(keyword_str, str) or not keyword_str.strip():\n",
    "        return None\n",
    "    \n",
    "    keywords = [k.strip() for k in keyword_str.split(\",\") if k.strip()]\n",
    "    return keywords if keywords else None\n",
    "\n",
    "def parse_countries(country_str):\n",
    "    if not isinstance(country_str, str) or not country_str.strip():\n",
    "        return None\n",
    "    \n",
    "    countries = [c.strip() for c in country_str.split(\",\") if c.strip()]\n",
    "    return countries if countries else None\n",
    "\n",
    "# used in mapping the movies and genres\n",
    "def parse_genres(genre_str):\n",
    "    if not isinstance(genre_str, str) or not genre_str.strip():\n",
    "        return []\n",
    "    return [g.strip() for g in genre_str.split(\",\") if g.strip()]\n",
    "\n",
    "def parse_companies(companies_str):\n",
    "    if not isinstance(companies_str, str) or not companies_str.strip():\n",
    "        return []\n",
    "    return [c.strip() for c in companies_str.split(\",\") if c.strip()]\n",
    "\n",
    "def parse_people(people_str):\n",
    "    if not isinstance(people_str, str) or not people_str.strip():\n",
    "        return []\n",
    "    return [p.strip() for p in people_str.split(\",\") if p.strip()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01b9f63",
   "metadata": {},
   "source": [
    "### Making Movie Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6e06ce1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2858/2858 [00:38<00:00, 74.09it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create Movie Nodes\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "csv_path = r\"C:\\Users\\anura\\OneDrive\\Desktop\\NPN Hackathon\\movie_metadata\\movie_meta_data.csv\"\n",
    "movies_df = pd.read_csv(csv_path)\n",
    "with driver.session(database=database) as session:\n",
    "    printed = 0\n",
    "    for _, row in tqdm(movies_df.iterrows(), total=len(movies_df)):\n",
    "        try:\n",
    "            imdb_raw = row.get(\"imdbid\")\n",
    "            if pd.isna(imdb_raw):\n",
    "                continue\n",
    "            imdb_id = str(imdb_raw).strip()\n",
    "            if not imdb_id:\n",
    "                continue\n",
    "\n",
    "            # basic fields\n",
    "            title = str(row.get(\"title\")).strip() if pd.notna(row.get(\"title\")) else None\n",
    "            year = row.get(\"year\", None)\n",
    "            # normalize numeric columns\n",
    "            def to_int(v):\n",
    "                if v is None or (isinstance(v, float) and pd.isna(v)):\n",
    "                    return None\n",
    "                try:\n",
    "                    return int(v)\n",
    "                except Exception:\n",
    "                    try:\n",
    "                        return int(float(v))\n",
    "                    except Exception:\n",
    "                        return None\n",
    "            def to_float(v):\n",
    "                if v is None or (isinstance(v, float) and pd.isna(v)):\n",
    "                    return None\n",
    "                try:\n",
    "                    return float(v)\n",
    "                except Exception:\n",
    "                    return None\n",
    "\n",
    "            metascore = to_int(row.get(\"metascore\", None))\n",
    "            imdb_rating = to_float(row.get(\"imdb user rating\", None))\n",
    "            imdb_votes = to_int(row.get(\"number of imdb user votes\", None))\n",
    "            budget = to_int(row.get(\"budget\", None))\n",
    "\n",
    "            # parse and serialize dicts -> JSON strings (safe primitive)\n",
    "            akas_dict = parse_akas_to_dict(row.get(\"akas\", None))\n",
    "            akas_json = json.dumps(akas_dict, ensure_ascii=False) if akas_dict else None\n",
    "\n",
    "            age_dict = parse_age_restrictions_to_dict(row.get(\"age restrict\", None))\n",
    "            age_json = json.dumps(age_dict, ensure_ascii=False) if age_dict else None\n",
    "\n",
    "            # lists\n",
    "            countries = parse_countries(row.get(\"countries\", None))\n",
    "            keywords = parse_keywords(row.get(\"keywords\", None))\n",
    "            taglines = parse_taglines(row.get(\"taglines\", None))\n",
    "            plot = row.get(\"plot\", None) if pd.notna(row.get(\"plot\", None)) else None\n",
    "\n",
    "            params = {\n",
    "                \"imdb_id\": imdb_id,\n",
    "                \"title\": title,\n",
    "                \"year\": year,\n",
    "                \"akas\": akas_json,\n",
    "                \"metascore\": metascore,\n",
    "                \"imdb_rating\": imdb_rating,\n",
    "                \"imdb_votes\": imdb_votes,\n",
    "                \"budget\": budget,\n",
    "                \"countries\": countries,\n",
    "                \"age_restrictions\": age_json,\n",
    "                \"keywords\": keywords,\n",
    "                \"taglines\": taglines,\n",
    "                \"plot\": plot\n",
    "            }\n",
    "\n",
    "            session.execute_write(create_movie, **params)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error preparing {row.get('imdbid')}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bafe7a0",
   "metadata": {},
   "source": [
    "### Making and connecting Genre Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "84bf7577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique genres: 27\n",
      "First 10: ['Action', 'Romance', 'Musical', 'Thriller', 'Drama', 'Animation', 'Talk-Show', 'Music', 'Game-Show', 'Film-Noir']\n"
     ]
    }
   ],
   "source": [
    "# Make genre nodes\n",
    "\n",
    "all_genres = set()\n",
    "\n",
    "for genres in movies_df[\"genres\"].dropna():\n",
    "    for g in str(genres).split(\",\"):\n",
    "        g = g.strip()\n",
    "        if g:\n",
    "            all_genres.add(g)\n",
    "\n",
    "print(\"Total unique genres:\", len(all_genres))\n",
    "print(\"First 10:\", list(all_genres)[:10])\n",
    "\n",
    "with driver.session(database=database) as session:\n",
    "    for genre in all_genres:\n",
    "        session.execute_write(create_genre, genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cce55550",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2858/2858 [00:23<00:00, 121.23it/s]\n"
     ]
    }
   ],
   "source": [
    "# Connecting Movie and Genre Nodes\n",
    "\n",
    "with driver.session(database=database) as session:\n",
    "    for _, row in tqdm(movies_df.iterrows(), total=len(movies_df)):\n",
    "        imdb_raw = row.get(\"imdbid\")\n",
    "        if pd.isna(imdb_raw):\n",
    "            continue\n",
    "        imdb_id = str(imdb_raw).strip()\n",
    "        if not imdb_id:\n",
    "            continue\n",
    "\n",
    "        genres = parse_genres(row.get(\"genres\", None))\n",
    "        if genres:\n",
    "            try:\n",
    "                session.execute_write(connect_movie_genres, imdb_id, genres)\n",
    "            except Exception as e:\n",
    "                print(f\"Error connecting genres for {imdb_id}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc66e239",
   "metadata": {},
   "source": [
    "### Making and connecting Award Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6a4a7f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Academy Awards adapted screenplay',\n",
      "       'Academy Awards original screenplay', 'BAFTA nominations',\n",
      "       'Golden Globe Award for Best Screenplay',\n",
      "       'Writers Guild Awards Winners & Nominees 2020-2013'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Create Movie Award Nodes\n",
    "csv_path = r\"C:\\Users\\anura\\OneDrive\\Desktop\\NPN Hackathon\\movie_metadata\\screenplay_awards.csv\"\n",
    "awards_df = pd.read_csv(csv_path)\n",
    "award_columns = awards_df.columns[1:]\n",
    "print(award_columns)\n",
    "\n",
    "with driver.session(database=database) as session:\n",
    "    for award_name in award_columns:\n",
    "        session.execute_write(create_award, award_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6603aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 462/462 [00:21<00:00, 21.93it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "ALLOWED_REL_TYPES = {\"WON\", \"NOMINATED_FOR\"}\n",
    "\n",
    "def connect_movie_award(tx, imdb_id, title, award_name, rel_type=\"WON\"):\n",
    "    if rel_type not in ALLOWED_REL_TYPES:\n",
    "        raise ValueError(f\"Invalid rel_type: {rel_type}\")\n",
    "\n",
    "    # Use MERGE for both nodes and MERGE the relationship.\n",
    "    # Note: rel_type is inserted into the query string after validation.\n",
    "    query = f\"\"\"\n",
    "    MERGE (m:Movie {{imdb_id: $imdb_id}})\n",
    "      ON CREATE SET m.title = $title\n",
    "    MERGE (a:Award  {{name: $award_name}})\n",
    "    MERGE (m)-[:{rel_type}]->(a)\n",
    "    \"\"\"\n",
    "    tx.run(query, imdb_id=imdb_id, title=title, award_name=award_name)\n",
    "\n",
    "csv_path = r\"C:\\Users\\anura\\OneDrive\\Desktop\\NPN Hackathon\\movie_metadata\\screenplay_awards.csv\"\n",
    "awards_df = pd.read_csv(csv_path)\n",
    "\n",
    "movie_col = awards_df.columns[0]\n",
    "award_columns = list(awards_df.columns[1:])\n",
    "\n",
    "nomination_awards = {\n",
    "    \"BAFTA nominations\",\n",
    "    \"Writers Guild Awards Winners & Nominees 2020-2013\"\n",
    "}\n",
    "\n",
    "with driver.session(database=database) as session:\n",
    "    for _, row in tqdm(awards_df.iterrows(), total=len(awards_df)):\n",
    "        movie_str = row[movie_col]\n",
    "        if not isinstance(movie_str, str) or \"_\" not in movie_str:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            title, imdb_id = movie_str.rsplit(\"_\", 1)\n",
    "            title = title.strip()\n",
    "            imdb_id = imdb_id.strip()\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        for award_name in award_columns:\n",
    "            value = row[award_name]\n",
    "            if pd.isna(value) or str(value).strip() == \"\":\n",
    "                continue  # no award info\n",
    "\n",
    "            rel_type = \"NOMINATED_FOR\" if award_name in nomination_awards else \"WON\"\n",
    "\n",
    "            try:\n",
    "                session.execute_write(connect_movie_award, imdb_id, title, award_name, rel_type)\n",
    "            except Exception as e:\n",
    "                print(f\"Error linking {title} ({imdb_id}) to {award_name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f646fdb",
   "metadata": {},
   "source": [
    "### Connecting Production companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6552ddfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2858/2858 [01:55<00:00, 24.67it/s]\n"
     ]
    }
   ],
   "source": [
    "csv_path = r\"C:\\Users\\anura\\OneDrive\\Desktop\\NPN Hackathon\\movie_metadata\\movie_meta_data.csv\"\n",
    "movies_df = pd.read_csv(csv_path)\n",
    "\n",
    "with driver.session(database=database) as session:\n",
    "    for _, row in tqdm(movies_df.iterrows(), total=len(movies_df)):\n",
    "        imdb_raw = row.get(\"imdbid\")\n",
    "        if pd.isna(imdb_raw):\n",
    "            continue\n",
    "        imdb_id = str(imdb_raw).strip()\n",
    "        if not imdb_id:\n",
    "            continue\n",
    "\n",
    "        companies = parse_companies(row.get(\"production companies\", None))\n",
    "        for company in companies:\n",
    "            try:\n",
    "                session.execute_write(connect_movie_company, imdb_id, company)\n",
    "            except Exception as e:\n",
    "                print(f\"Error linking {imdb_id} to {company}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce266170",
   "metadata": {},
   "source": [
    "### Making and connecting Person Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f841d94c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2858/2858 [2:33:26<00:00,  3.22s/it]  \n"
     ]
    }
   ],
   "source": [
    "csv_path = r\"C:\\Users\\anura\\OneDrive\\Desktop\\NPN Hackathon\\movie_metadata\\movie_meta_data.csv\"\n",
    "movies_df = pd.read_csv(csv_path)\n",
    "\n",
    "role_mapping = {\n",
    "    \"producers\": \"PRODUCED_BY\",\n",
    "    \"writers\": \"WRITTEN_BY\",\n",
    "    \"directors\": \"DIRECTED_BY\",\n",
    "    \"casting directors\": \"CASTING_DIRECTED_BY\",\n",
    "    \"cast\": \"ACTED_IN\"\n",
    "}\n",
    "\n",
    "with driver.session(database=database) as session:\n",
    "    for _, row in tqdm(movies_df.iterrows(), total=len(movies_df)):\n",
    "        imdb_raw = row.get(\"imdbid\")\n",
    "        if pd.isna(imdb_raw):\n",
    "            continue\n",
    "        imdb_id = str(imdb_raw).strip()\n",
    "        if not imdb_id:\n",
    "            continue\n",
    "\n",
    "        for col, rel in role_mapping.items():\n",
    "            people = parse_people(row.get(col, None))\n",
    "            for person in people:\n",
    "                try:\n",
    "                    session.execute_write(\n",
    "                        connect_movie_person,\n",
    "                        imdb_id,\n",
    "                        person,\n",
    "                        rel,\n",
    "                        role=col[:-1] if col.endswith(\"s\") else col\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    print(f\"Error linking {person} in {col} for {imdb_id}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4742aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Fix \"cas\" role to \"cast\" or \"actor\"\n",
    "MATCH (p:Person)\n",
    "WHERE p.role = 'cas'\n",
    "SET p.role = 'actor';\n",
    "\n",
    "\n",
    "MATCH (p:Person)\n",
    "WHERE p.role = 'casting director'\n",
    "SET p.role = 'Casting Director';\n",
    "\n",
    "MATCH (p:Person)\n",
    "WHERE p.role = 'producer'\n",
    "SET p.role = 'Producer';\n",
    "\n",
    "MATCH (p:Person)\n",
    "WHERE p.role = 'writer'\n",
    "SET p.role = 'Writer';\n",
    "\n",
    "MATCH (p:Person)\n",
    "WHERE p.role = 'director'\n",
    "SET p.role = 'Director';\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2336e1",
   "metadata": {},
   "source": [
    "### Making and Connecting Review Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a88a6c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: CUDA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "  0%|          | 9/21024 [00:02<1:07:15,  5.21it/s]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "100%|██████████| 21024/21024 [25:44<00:00, 13.61it/s] \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "csv_path = r\"C:\\Users\\anura\\OneDrive\\Desktop\\NPN Hackathon\\movie_metadata\\metacritic_reviews_cut_versions.csv\"\n",
    "review_df = pd.read_csv(csv_path)\n",
    "\n",
    "\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "print(\"Using device:\", \"CUDA\" if device == 0 else \"CPU\")\n",
    "\n",
    "sentiment_analyzer = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
    "    device=device\n",
    ")\n",
    "\n",
    "with driver.session(database=database) as session:\n",
    "    for idx, row in tqdm(review_df.iterrows(), total=len(review_df)):\n",
    "        review_text = str(row[\"text\"])\n",
    "        score = row.get(\"score\", None)\n",
    "        imdb_id = str(row[\"imdbid\"])\n",
    "\n",
    "        result = sentiment_analyzer(review_text, truncation=True, max_length=512)[0]\n",
    "        sentiment = result[\"label\"].lower()\n",
    "        sentiment_score = float(result[\"score\"])\n",
    "\n",
    "        review_id = f\"{imdb_id}_review_{idx}\"\n",
    "\n",
    "        session.execute_write(\n",
    "            create_review,\n",
    "            imdb_id,\n",
    "            review_id,\n",
    "            review_text,\n",
    "            \"Metacritic\",\n",
    "            score,\n",
    "            sentiment,\n",
    "            sentiment_score\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75f41702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Suggest me any 3 movies.\n",
      "A: Here are three movie suggestions for you: A Night at the Roxbury (1998), At First Sight (1999), and The Avengers (1998).\n"
     ]
    }
   ],
   "source": [
    "from langchain_neo4j import Neo4jGraph, GraphCypherQAChain\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.tools import Tool\n",
    "from langchain.prompts import PromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "def clean_response(output: str) -> str:\n",
    "    import re\n",
    "    return re.sub(r\"<think>.*?</think>\", \"\", output, flags=re.DOTALL).strip()\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"deepseek-r1-distill-llama-70b\",\n",
    "    temperature=0.0,\n",
    "    max_tokens=None,\n",
    "    max_retries=2,\n",
    "    timeout=None,\n",
    ")\n",
    "\n",
    "cypher_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\", \"schema\"],\n",
    "    template=\"\"\"\n",
    "You are a Cypher expert. Given a natural language question and the database schema, \n",
    "generate a safe and correct Cypher query.\n",
    "\n",
    "Schema:\n",
    "{schema}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Return only the Cypher query, nothing else.\n",
    "\"\"\",\n",
    ")\n",
    "\n",
    "graph = Neo4jGraph(\n",
    "    url=\"bolt://localhost:7687\",\n",
    "    username=\"neo4j\",\n",
    "    password=\"12345678\",\n",
    "    database=\"moviesdb\"\n",
    ")\n",
    "\n",
    "graph_qa = GraphCypherQAChain.from_llm(\n",
    "    llm=llm,\n",
    "    graph=graph,\n",
    "    cypher_prompt=cypher_prompt,\n",
    "    response_format=\"text\",\n",
    "    allow_dangerous_requests=True\n",
    ")\n",
    "\n",
    "# --- Wrap in Tool\n",
    "def neo4j_tool_fn(question: str) -> str:\n",
    "    try:\n",
    "        raw = graph_qa.run(question)\n",
    "        return clean_response(raw)\n",
    "    except Exception as e:\n",
    "        return f\"[Neo4j tool error] {type(e).__name__}: {e}\"\n",
    "\n",
    "neo4j_tool = Tool(\n",
    "    name=\"Neo4jGraphQA\",\n",
    "    func=neo4j_tool_fn,\n",
    "    description=\"Answer questions about movies/people/awards using the Neo4j graph. Generate correct Cypher queries based on schema.\"\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    q = \"Suggest me any 3 movies.\"\n",
    "    print(\"Q:\", q)\n",
    "    print(\"A:\", neo4j_tool_fn(q))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c22968",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import Tool\n",
    "\n",
    "neo4j_tool = Tool(\n",
    "    name=\"Neo4jGraphQA\",\n",
    "    func=graph_qa.run,\n",
    "    description=\"Answer questions about movies, genres, and awards using the Neo4j graph database.\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
